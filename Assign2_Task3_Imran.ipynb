{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKUiP5y5trxNfD6B00TW+3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Name:M.Imran Saeed**\n","# **Assignment 2**\n","# **Task 3**\n","1. Write a Python program that allows users to select a query image from folder\n","   “query_images” and retrieve the top N similar images from a local folder named “images_database”. Put N=4 for this task so the system must spit out 4 similar images from the given folder closely similar to your query image. Your program should use a pre-trained CNN model (e.g., VGG16 or ResNet) for feature extraction and a similarity metric (e.g., Euclidean distance) for retrieval.\n","2. Ensure that your program can handle images in various formats (e.g., JPG,\n","   PNG, JPEG).\n","3. Provide clear instructions on how to run your program and demonstrate its\n","   functionality using sample query images.\n","   This coding task allows you to apply your understanding of CNN-based feature extraction to build a practical image retrieval system, like the technologies used by Google Lens."],"metadata":{"id":"1H200QllKAXu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wc2CB9Z5ue9Y","executionInfo":{"status":"ok","timestamp":1698343072549,"user_tz":-300,"elapsed":3849,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"7fa6071a-6aef-41af-cd2d-58a8d852dc4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount ('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data_assign_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qk94i8UzuflY","executionInfo":{"status":"ok","timestamp":1698343074390,"user_tz":-300,"elapsed":2,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"e7d9e068-76c8-42b2-b775-9c167cd90b09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data_assign_2\n"]}]},{"cell_type":"markdown","source":["**Importing necessary libraries/modules:**\n","\n","**os:** Operating system interface for file and directory operations.\n","\n","**cv2:** OpenCV (Open Source Computer Vision Library) for image processing and computer vision.\n","\n","**numpy:** NumPy is used for numerical operations, particularly for handling arrays and matrices.\n","\n","**tensorflow.keras.applications.VGG16:** Importing the VGG16 model, a popular deep convolutional neural network (CNN) for image classification tasks, from TensorFlow's Keras applications.\n","\n","**tensorflow.keras.applications.vgg16.preprocess_input:** A function for preprocessing images for VGG16 model.\n","\n","**sklearn.metrics.pairwise.euclidean_distances:** Importing a function to calculate the pairwise Euclidean distances between points in a dataset.\n","matplotlib.pyplot: Importing Matplotlib for data visualization."],"metadata":{"id":"cVDiIuB3t8UG"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt"],"metadata":{"id":"1accpwMVuf6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**query_folder is set to the string \"query_images\".** This variable likely represents the folder path where query images are located.\n","\n","**Image_databases is also set to the string \"query_images\".** This variable appears to represent the folder path where image databases or reference images are located. It's worth noting that both query_folder and Image_databases are set to the same path in your snippet. In a real-world scenario, these paths would typically point to different folders containing query images and reference images.\n","\n","**N is set to the integer 4.**This variable's purpose is not clear from the provided code snippet. It could represent a count or a threshold value, but its meaning and usage depend on the context of the larger script."],"metadata":{"id":"Rv1BVRKuuT4s"}},{"cell_type":"code","source":["query_folder = \"query_images\"\n","Image_databases = \"query_images\"\n","N = 4"],"metadata":{"id":"7CiJZJFfFDXl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model Initialization:** The code first initializes a VGG16 model by loading pre-trained weights from the 'imagenet' dataset. This model is configured to exclude the top classification layers, and the input shape is set to (224, 224, 3). This means that the model expects images with a resolution of 224x224 pixels and three color channels (RGB).\n","\n","**extract_features Function:** This function is defined to extract features from an image using the pre-trained VGG16 model. It takes two arguments:\n","\n","**image_path:** The file path to the input image that you want to extract features from.\n","**model:** The pre-trained VGG16 model.\n","\n","**Loading and Preprocessing Image:** Inside the **extract_features function:**\n","\n","It loads the image located at the **image_path** using OpenCV **(cv2.imread)**.\n","\n","If the image cannot be loaded (e.g., if the file doesn't exist or is in an unsupported format), it prints an error message and returns **None**.\n","\n","It converts the image from BGR format to RGB format using **cv2.cvtColor**.\n","Resizes the image to match the expected input shape of the VGG16 model (224x224 pixels).\n","\n","Preprocesses the image using **preprocess_input**, which performs normalization and preprocessing specific to the VGG16 model.\n","Expands the dimensions of the image to make it compatible with the model's input shape. The resulting image has shape (1, 224, 224, 3), where the first dimension is for batching (batch size 1).\n","\n","**Feature Extraction:** The preprocessed image is then passed through the VGG16 model using model.predict(image). This step extracts a feature representation of the input image using the layers of the pre-trained model.\n","\n","**Return Features:** The extracted features are returned from the function."],"metadata":{"id":"eKKQY0u2uhzO"}},{"cell_type":"code","source":["model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","def extract_features(image_path, model):\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Error: Unable to load image from {image_path}\")\n","        return None\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (224, 224))  # Adjust the size to match the pre-trained model\n","    image = preprocess_input(image)  # Preprocess the image for the chosen model\n","    image = np.expand_dims(image, axis=0)\n","    features = model.predict(image)\n","    return features"],"metadata":{"id":"ui5lsiaCxeKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_image = os.path.join(query_folder, \"query_images\")  # Replace with the name of your query image\n","\n","query_features = extract_features(query_image, model)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swNK650kxl-X","executionInfo":{"status":"ok","timestamp":1698343389081,"user_tz":-300,"elapsed":3,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"a01dd23c-6028-4722-e06d-e14b93237c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: Unable to load image from query_images/query_images\n"]}]},{"cell_type":"markdown","source":["The provided code snippet appears to iterate through a directory containing image files (assumed to be in the \"query_images\" folder) and computes similarities between those images and a reference image using previously defined functions. Let's break it down step by step:\n","\n","**similarities = {}:** This line initializes an empty dictionary called similarities to store the computed similarities.\n","\n","**for filename in os.listdir(\"query_images\")::** This line starts a loop to iterate over the files in the \"query_images\" directory using os.listdir(). It will consider all files (images) in that directory.\n","\n","**if filename.endswith((\".jpg\", \".jpeg\", \".png\"))::** This line checks if the current filename has one of the specified image file extensions (\".jpg\", \".jpeg\", or \".png\"). If it does, the code proceeds with the following operations for that file; otherwise, it skips non-image files.\n","\n","**image_path = os.path.join(Image_databases, filename):** This line constructs the full path to the image by joining the Image_databases (which you've set as \"query_images\") with the current filename. This assumes that the reference images are stored in the \"query_images\" directory.\n","\n","**image_features = extract_features(image_path, model):** For each image, it calls the extract_features function (presumably defined earlier in the code) to extract features from the image using the provided VGG16 model (model). The extracted features are stored in the image_features variable.\n","\n","After running this loop, you will have extracted features for each image in the \"query_images\" directory that matches the specified file extensions."],"metadata":{"id":"ofNaSTg4w9Vd"}},{"cell_type":"code","source":["similarities = {}\n","\n","for filename in os.listdir(\"query_images\"):\n","    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n","        image_path = os.path.join(Image_databases, filename)\n","        image_features = extract_features(image_path, model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SQ9Kt-1xYJZ","executionInfo":{"status":"ok","timestamp":1698343402731,"user_tz":-300,"elapsed":4038,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"b8681038-cbcc-4391-fac9-359718619d33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 757ms/step\n","1/1 [==============================] - 1s 562ms/step\n","1/1 [==============================] - 1s 545ms/step\n","1/1 [==============================] - 1s 547ms/step\n","1/1 [==============================] - 1s 560ms/step\n","1/1 [==============================] - 1s 709ms/step\n"]}]},{"cell_type":"markdown","source":["The provided code is part of a larger script for computing similarity scores between a query image and a set of database images.\n","\n","**if query_features is not None and image_features is not None::** This conditional statement checks if both the query_features and image_features variables are not None. This is a check to ensure that feature extraction was successful for both the query image and the database image.\n","\n","**similarity_score = euclidean_distances(query_features.reshape(1, -1), image_features.reshape(1, -1)):** If feature extraction was successful for both images, this line calculates the Euclidean distance between the feature vectors of the query image and the database image. It uses the euclidean_distances function from the sklearn.metrics.pairwise module. This distance score is a measure of dissimilarity, where lower values indicate greater similarity.\n","\n","**if not np.isnan(similarity_score).any()::** This line checks if any of the values in the similarity_score array are not NaN (not-a-number). This check is essential because if feature extraction fails for any image, the similarity score could be NaN, and you want to skip such cases.\n","\n","**similarity_score = similarity_score[0][0]:** If the similarity_score is not NaN, this line extracts the actual similarity score (the single value in the similarity_score array) and assigns it to the similarity_score variable.\n","\n","**similarities[filename] = similarity_score:** This line stores the computed similarity score in the similarities dictionary, where the filename of the database image is the key, and the similarity score is the value.\n","\n","If any of the above conditions are not met, it means that either feature extraction failed for the query image, the database image, or the similarity score was NaN. In such cases:\n","\n","The **first else** block is used to handle cases where feature extraction failed for either the query or database image. It prints a message indicating that the image is skipped.\n","\n","The **second else** block handles cases where the similarity score is NaN (e.g., due to failed feature extraction), and it also prints a message indicating that the image is skipped."],"metadata":{"id":"Pj7kgnMvyIvo"}},{"cell_type":"code","source":["if query_features is not None and image_features is not None:\n","    similarity_score = euclidean_distances(query_features.reshape(1, -1), image_features.reshape(1, -1))\n","\n","    if not np.isnan(similarity_score).any():\n","        similarity_score = similarity_score[0][0]\n","        similarities[filename] = similarity_score\n","    else:\n","        # Handle cases where the similarity_score is NaN (e.g., due to failed feature extraction)\n","        print(f\"Skipping image {filename} due to invalid feature extraction.\")\n","else:\n","    # Handle cases where feature extraction failed for either the query or database image\n","    print(f\"Skipping image {filename} due to failed feature extraction.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4JPs-oy4vmu1","executionInfo":{"status":"ok","timestamp":1698343408228,"user_tz":-300,"elapsed":522,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"42b7250d-32c8-4431-87e7-98384d8dfc82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping image 97ac23148b.jpg due to failed feature extraction.\n"]}]},{"cell_type":"code","source":["sorted_similarities = {k: v for k, v in sorted(similarities.items(), key=lambda item: item[1])}\n","top_similar_images = list(sorted_similarities.keys())[:N]\n"],"metadata":{"id":"gq0cZuD00zCn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The provided code is responsible for displaying the top N similar images from the similarities dictionary, which presumably contains similarity scores between a query image and a set of database images.\n","\n","**Here's what the code does:**\n","\n","**sorted_similarities = {k: v for k, v in sorted(similarities.items(), key=lambda item: item[1])}:**\n","\n","This line sorts the similarities dictionary based on the similarity scores (values) in ascending order. It creates a new dictionary sorted_similarities where the keys are the filenames of the images, and the values are the similarity scores, sorted from the lowest to the highest.\n","\n","**top_similar_images = list(sorted_similarities.keys())[:N]:** This line extracts the top N images with the lowest similarity scores from the sorted_similarities dictionary. These are the images that are most similar to the query image. The top_similar_images variable contains a list of filenames of these top N similar images.\n","\n","**print(\"Top {} similar images:\".format(N)):** This line prints a message indicating that it's about to display the top N similar images.\n","\n","The following loop then iterates through the top N image filenames (image variable) and does the following for each image:\n","\n","**image_path = os.path.join(Image_databases, image):** Constructs the full path to the image in the database by joining the Image_databases (which you've set as \"query_images\") with the current image filename.\n","\n","**img = cv2.imread(image_path):** Reads and loads the image using OpenCV.\n","**img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB):** Converts the image from BGR format to RGB format for displaying with Matplotlib.\n","**plt.imshow(img):** Displays the image using Matplotlib.\n","**plt.axis('OFF'):** Turns off the axis labels in the Matplotlib plot.\n","**plt.show():** Shows the image."],"metadata":{"id":"vzJ7Az75zIYZ"}},{"cell_type":"code","source":["print(\"Top {} similar images:\".format(N))\n","\n","for image in top_similar_images:\n","    image_path = os.path.join(Image_databases, image)\n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.imshow(img)\n","    plt.axis('OFF')\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5QifNV7GAIw","executionInfo":{"status":"ok","timestamp":1698343414372,"user_tz":-300,"elapsed":542,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"}},"outputId":"32c44980-8b9c-472e-c061-4e6b0c8e6bd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 4 similar images:\n"]}]},{"cell_type":"markdown","source":["This code will display the top N images that are most similar to the query image based on their similarity scores. It does this by sorting the images by similarity, selecting the top N, and displaying them using Matplotlib."],"metadata":{"id":"a6KoVmKhymY_"}}]}