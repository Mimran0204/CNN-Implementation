{"cells":[{"cell_type":"markdown","metadata":{"id":"r-LyPkB8ism7"},"source":["# **Name: Muhammad Imran Saeed**\n","#**Assignment No 2**\n","# **Task 2**\n","Now this time do Task 1 again by fine-tuning the pre-trained model of your choice. Contents of week 6 and week 7 are focused on the concept of transfer learning so the main objective of this task is to solidify and reinforce your core concepts of transfer learning and how to fine-tune a pre-trained model trained on different dataset for your own custom dataset. Observe the accuracy of the model and compare it with the accuracy of the model in task 1."]},{"cell_type":"markdown","source":["# **Solution**\n","\n","**from google.colab import drive:** This line imports the drive module from the google.colab library. This module provides functions for working with Google Drive in Colab.\n","\n","**drive.mount('/content/drive'):** This line mounts the Google Drive at the specified directory, which is **/content/drive** in the Colab notebook. After executing this line, we'll be prompted to authenticate and give permission for Colab to access your Google Drive. Once authorized, The Google Drive will be accessible as if it were a local directory in the Colab environment. After mounted the Google Drive using this code, we can access the files and folders, read and write data, and perform various data analysis or machine learning tasks that involve your Google Drive data within your Colab notebook. It's a convenient way to work with cloud-based data and files in a Colab environment.\n","\n"],"metadata":{"id":"ypWv_YaNd2Hj"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fvJe5e2pgpx7","outputId":"ef226346-8ae2-404a-a5de-e0a2a4498c20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount ('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488,"status":"ok","timestamp":1698315818681,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"},"user_tz":-300},"id":"IxH__q8lgoF6","outputId":"ab16816a-4304-48ab-dfd3-d85974d2339a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/data_assign_2\n"]}],"source":["%cd /content/drive/MyDrive/data_assign_2"]},{"cell_type":"markdown","source":["This code shows the beginning of a script for building a deep learning model for image classification using TensorFlow and the VGG16 architecture.\n","\n","**Importing necessary libraries:**\n","This part of the code imports TensorFlow and various components required for building a deep learning model, including image data preprocessing tools, the VGG16 architecture, layers, models, and the Adam optimizer. Next, it's expected that you would define and configure an image data generator for preprocessing your image data. However, the code you provided is incomplete, and there is no configuration for the data generator.\n","\n","**base_model = VGG16(weights='imagenet', include_top=False)**\n","This line loads the VGG16 model pretrained on the ImageNet dataset. The include_top=False argument means that the top classification layers of VGG16 (fully connected layers) are excluded, and you will add your own custom classification layers.\n","\n","Here, we are adding some custom layers on top of the VGG16 model. The first thing is to apply a global average pooling layer, followed by a dense (fully connected) layer with ReLU activation, and then another dense layer with a softmax activation function. The number of units in the last dense layer (num_classes) should be equal to the number of classes in your classification problem.\n","\n","**model = Model(inputs=base_model.input, outputs=predictions)**\n","This line creates a new model that combines the base VGG16 model with your custom classification layers. The input is set to the input of the VGG16 model, and the output is set to your custom predictions.\n","\n","**model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])**\n","The model, specifying the optimizer (Adam with a learning rate of 0.0001), the loss function (categorical cross-entropy, typically used for classification problems), and the evaluation metric (accuracy)."],"metadata":{"id":"EZvGI1Q0eJLI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OiExhxMjSuo"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qrri9jfUl_dG"},"outputs":[],"source":["train_dir= 'hand_written_digits'\n","valid_dir = 'hand_written_digits'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTC4Gn_FmPQQ"},"outputs":[],"source":["image_size = (100, 100)\n","batch_size = 32"]},{"cell_type":"markdown","source":["The provided code sets up an **ImageDataGenerator** object for data augmentation. Data augmentation is a technique commonly used in deep learning for image classification tasks to artificially increase the size of the training dataset by applying various random transformations to the original images. This helps improve the model's generalization and robustness.\n","\n","**rescale=1.0 / 255.0:** This parameter rescales the pixel values of the input images. In this case, it divides each pixel value by 255.0, which is a common practice to ensure that pixel values are in the range [0, 1].\n","\n","**rotation_range=20:** This parameter specifies the range within which random rotations (in degrees) can be applied to the images. In this case, images can be randomly rotated up to 20 degrees in either direction.\n","\n","**width_shift_range=0.2:** This parameter defines the range for random horizontal shifts (as a fraction of the total width of the image). A value of 0.2 means that images can be shifted horizontally by up to 20% of their width in either direction.\n","\n","**height_shift_range=0.2:** Similar to width_shift_range, this parameter defines the range for random vertical shifts (as a fraction of the total height of the image). A value of 0.2 means that images can be shifted vertically by up to 20% of their height in either direction.\n","\n","**shear_range=0.2:** This parameter controls the shear intensity. Shear transforms slant the shapes of objects in the images. A value of 0.2 means that shear transformations can be applied up to 20%.\n","\n","**zoom_range=0.2:** The zoom range specifies the range for random zooming in or out of the images. A value of 0.2 means that images can be zoomed in or out by up to 20%.\n","\n","**horizontal_flip=False:** This parameter controls whether random horizontal flips are applied to the images. If set to True, images may be horizontally flipped with a 50% probability.\n","\n","**fill_mode='nearest':** The fill mode determines how pixel values are filled when the above transformations cause empty areas in the image. In this case, 'nearest' indicates that the nearest pixels are used to fill the empty regions.\n"],"metadata":{"id":"K__ShsGYkGKO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqSwpDG5mtVw"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","    rescale=1.0 / 255.0,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=False,\n","    fill_mode='nearest'\n",")"]},{"cell_type":"markdown","source":["The provided code is setting up an image data generator and a generator for the training dataset. It appears to be part of a deep learning pipeline for image classification using TensorFlow and Keras. Let's break down the code:\n","\n","**valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0):** This line creates an ImageDataGenerator object for preprocessing the validation data. It performs pixel value rescaling by dividing each pixel value by 255.0 to ensure that pixel values are in the range [0, 1].\n","\n","**train_generator = train_datagen.flow_from_directory(train_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical':** This line sets up a generator for the training data by using the flow_from_directory method. Here's what each argument does:\n","\n","**train_dir:** This should be a directory containing the training images. The generator will read and preprocess images from this directory.\n","\n","**target_size:** This is the size to which the images will be resized during preprocessing. It's typically set to a tuple, e.g., image_size = (224, 224), to ensure that all images are the same size for consistency.\n","\n","**batch_size:** This parameter specifies the batch size for training. It determines how many images are processed in each training iteration.\n","\n","**class_mode='categorical':** This parameter specifies that the data is used for categorical classification. It means that the generator will expect the images to be organized into subdirectories, where each subdirectory represents a class, and it will generate labels accordingly. This is common for image classification tasks."],"metadata":{"id":"6TFH1V45l-pl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1698315832841,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"},"user_tz":-300},"id":"vxDjVHaUmw_p","outputId":"318e64c9-78f5-4511-b3a3-171a79375e18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4668 images belonging to 3 classes.\n"]}],"source":["\n","valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")"]},{"cell_type":"markdown","source":["The provided code sets up an image data generator for the validation dataset using the **flow_from_directory method**. It is a part of a deep learning pipeline for image classification. Here's what the code does:\n","\n","**valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical'):** This line creates a generator for the validation data by using the flow_from_directory method with the following arguments:\n","\n","**valid_dir:** This should be a directory containing the validation images. The generator will read and preprocess images from this directory.\n","\n","**target_size:** This parameter specifies the size to which the images will be resized during preprocessing. It's typically set to a tuple, e.g., image_size = (224, 224), to ensure that all images are the same size for consistency with the training data.\n","\n","**batch_size:** This parameter determines the batch size for validation. It specifies how many validation images are processed in each validation iteration.\n","\n","**class_mode='categorical':** This parameter specifies that the data is used for categorical classification. It means that the generator expects the images to be organized into subdirectories, where each subdirectory represents a class, and it generates labels accordingly. This is a common approach for image classification tasks."],"metadata":{"id":"Txn3FCAqmm4m"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698315834281,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"},"user_tz":-300},"id":"-c_HWZIFm36p","outputId":"8906c41e-4b48-4fa3-ea47-1ae83811a898"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4668 images belonging to 3 classes.\n"]}],"source":["valid_generator = valid_datagen.flow_from_directory(\n","    valid_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n"]},{"cell_type":"markdown","source":["**base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3)):**\n","\n","This line creates the VGG16 model with the following options:\n","\n","**weights='imagenet':**It loads pre-trained weights from the ImageNet dataset, which is a common practice for transfer learning in image classification tasks.\n","\n","**include_top=False:** This excludes the top (classification) layer of the VGG16 model, allowing you to add your custom classification layers.\n","\n","**input_shape=(100, 100, 3):** It specifies the input shape for your images. In this case, it's set to (100, 100, 3), which means that the model expects input images with a resolution of 100x100 pixels and three color channels (RGB)."],"metadata":{"id":"RoHpFfVRnPjv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVuf5xjum-ug"},"outputs":[],"source":["base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","predictions = Dense(3, activation='softmax')(x)"]},{"cell_type":"markdown","source":["The provided code constructs a new Keras model by combining the base VGG16 model with the custom classification layers you've defined earlier. Additionally, it freezes (sets as non-trainable) all the layers in the base VGG16 model. Let's break it down step by step:\n","\n","**model = Model(inputs=base_model.input, outputs=predictions):**\n","\n","This line creates a new Keras model (model) by specifying its inputs and outputs.\n","**inputs=base_model.input** sets the inputs of the new model to be the same as the inputs of the base_model, which is the VGG16 model with custom input shape.\n","\n","**outputs=predictions** sets the outputs of the new model to be the predictions generated by the custom layers you defined earlier. These custom layers are connected to the output of the base VGG16 model.\n","\n","**for layer in base_model.layers: layer.trainable = False:**\n","\n","This code iterates through all the layers in the base_model, which is the VGG16 model.\n","For each layer, it sets the **trainable** attribute to **False**. This effectively freezes all the layers in the base VGG16 model, preventing them from being updated during the training process."],"metadata":{"id":"-RHXXzCxnufA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJ48yvqYnEB6"},"outputs":[],"source":["model = Model(inputs=base_model.input, outputs=predictions)\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n"]},{"cell_type":"markdown","source":["**model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']):**\n","\n","This line compiles the model. It specifies:\n","**optimizer=Adam(learning_rate=0.0001):** The Adam optimizer with a learning rate of 0.0001 is used to update the model's weights during training.\n","**loss='categorical_crossentropy':** The categorical cross-entropy loss function, which is commonly used for multi-class classification tasks.\n","**metrics=['accuracy']:** During training, the model will track and report the classification accuracy as one of the evaluation metrics.\n","**epochs = 10:** This line sets the number of training epochs. The model will be trained for 10 complete passes through the training dataset.\n","\n","**history = model.fit(...):** This is the training loop that fits the model to the training data and validates it on the validation data. The key arguments are as follows:\n","\n","**train_generator:** This is the generator for the training dataset, which provides batches of training data during training.\n","steps_per_epoch=train_generator.samples // batch_size: This parameter determines how many steps (batches) are processed per training epoch. It's calculated based on the number of samples in the training dataset and the specified batch size.\n","\n","**validation_data=valid_generator:** This is the generator for the validation dataset, which provides batches of validation data for model evaluation during training.\n","\n","**validation_steps=valid_generator.samples // batch_size:** Similar to steps_per_epoch, this parameter determines how many steps (batches) are processed per validation epoch, calculated based on the validation dataset size and batch size.\n","\n","**epochs=epochs:** This specifies the number of training epochs, as previously defined.\n","\n","During training, the model's weights are updated using the optimizer (Adam) to minimize the categorical cross-entropy loss. The model's performance is evaluated on both the training and validation datasets, and the training progress (loss and accuracy) is stored in the history variable.\n","\n","After training for the specified number of epochs, the history object will contain information about the training process, such as the loss and accuracy at each epoch. This information can be used to analyze and visualize how the model's performance evolves over the training process."],"metadata":{"id":"XVpVgMtgoN82"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287780,"status":"ok","timestamp":1698323101134,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"},"user_tz":-300},"id":"YESiUt3Knh4K","outputId":"34193483-a47c-4129-d619-01dd973e1eaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","145/145 [==============================] - 691s 5s/step - loss: 0.8131 - accuracy: 0.8003 - val_loss: 0.5127 - val_accuracy: 0.9744\n","Epoch 2/10\n","145/145 [==============================] - 732s 5s/step - loss: 0.4023 - accuracy: 0.9717 - val_loss: 0.2636 - val_accuracy: 0.9836\n","Epoch 3/10\n","145/145 [==============================] - 731s 5s/step - loss: 0.2425 - accuracy: 0.9735 - val_loss: 0.1597 - val_accuracy: 0.9860\n","Epoch 4/10\n","145/145 [==============================] - 697s 5s/step - loss: 0.1632 - accuracy: 0.9793 - val_loss: 0.1097 - val_accuracy: 0.9869\n","Epoch 5/10\n","145/145 [==============================] - 693s 5s/step - loss: 0.1232 - accuracy: 0.9782 - val_loss: 0.0835 - val_accuracy: 0.9881\n","Epoch 6/10\n","145/145 [==============================] - 694s 5s/step - loss: 0.1000 - accuracy: 0.9812 - val_loss: 0.0680 - val_accuracy: 0.9884\n","Epoch 7/10\n","145/145 [==============================] - 696s 5s/step - loss: 0.0830 - accuracy: 0.9838 - val_loss: 0.0591 - val_accuracy: 0.9890\n","Epoch 8/10\n","145/145 [==============================] - 694s 5s/step - loss: 0.0742 - accuracy: 0.9860 - val_loss: 0.0517 - val_accuracy: 0.9890\n","Epoch 9/10\n","145/145 [==============================] - 732s 5s/step - loss: 0.0725 - accuracy: 0.9825 - val_loss: 0.0455 - val_accuracy: 0.9901\n","Epoch 10/10\n","145/145 [==============================] - 690s 5s/step - loss: 0.0647 - accuracy: 0.9840 - val_loss: 0.0427 - val_accuracy: 0.9905\n"]}],"source":["model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","epochs = 10\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // batch_size,\n","    validation_data=valid_generator,\n","    validation_steps=valid_generator.samples // batch_size,\n","    epochs=epochs\n",")\n"]},{"cell_type":"markdown","source":["**accuracy = model.evaluate(valid_generator)[1]:**\n","\n","model.evaluate(valid_generator) computes the model's loss and metrics on the validation dataset using the validation data generator (valid_generator).\n","\n","**[1]** accesses the second element of the list returned by model.evaluate, which corresponds to the accuracy. In the list, the first element is the loss, and the second element is the accuracy.\n","\n","**print(\"Fine-tuned Model Validation Accuracy: {:.2f}%\".format(accuracy * 100)**\n","\n","This line prints the validation accuracy in a human-readable format. It takes the accuracy value (a decimal between 0 and 1) and multiplies it by 100 to convert it to a percentage.\n","\n","**The \"{:.2f}%\".format(accuracy * 100)** part of the code formats the accuracy value to display two decimal places followed by a percentage sign.\n","So, the code calculates the validation accuracy of the fine-tuned model and then prints it as a percentage. This provides an indication of how well the model is performing on the validation dataset after the specified number of training epochs."],"metadata":{"id":"vmgInz-CperW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382381,"status":"ok","timestamp":1698324508577,"user":{"displayName":"Discover ComputerScience","userId":"11059772608898376359"},"user_tz":-300},"id":"07fsldqJK9hi","outputId":"60dbe80e-488c-45a5-8903-6d368969d7b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["146/146 [==============================] - 347s 2s/step - loss: 0.0430 - accuracy: 0.9904\n","Fine-tuned Model Validation Accuracy: 99.04%\n"]}],"source":["accuracy = model.evaluate(valid_generator)[1]\n","print(\"Fine-tuned Model Validation Accuracy: {:.2f}%\".format(accuracy * 100))"]},{"cell_type":"markdown","source":["So, the code calculates the validation accuracy of the fine-tuned model and then prints it as a percentage. This provides an indication of how well the model is performing on the validation dataset after the specified number of training epochs."],"metadata":{"id":"o46yPQ6Vp9eQ"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0fRbJXGwlKT6qEw5Xt3iX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}